{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Specify a directory to store NLTK data\n",
    "nltk.data.path.append('C:/nltk_data')  # You can change this path if needed\n",
    "\n",
    "# Download the necessary data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_18576\\2293607396.py\", line 2, in <module>\n",
      "    import spacy\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: ['real', 'time', 'animation', 'deformable', 'objects', 'compromise']\n",
      "Bigrams: ['real time', 'time animation', 'animation deformable', 'deformable objects', 'objects compromise']\n",
      "Trigrams: ['real time animation', 'time animation deformable', 'animation deformable objects', 'deformable objects compromise']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example list of agreement and disagreement lexicons\n",
    "agreement_lexicon = ['agree', 'agreeing', 'support', 'supporting', 'in favor of', 'yes', 'yes', 'certainly', 'definitely', 'sure']\n",
    "disagreement_lexicon = ['disagree', 'disagreeing', 'oppose', 'opposing', 'against', 'no', 'not', 'never', 'certainly not', 'definitely not']\n",
    "\n",
    "# Helper functions\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_ngrams(text, n=1):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using spaCy and generates n-grams.\n",
    "    \"\"\"\n",
    "    # Tokenize the text using spaCy\n",
    "    doc = nlp(text.lower())  # spaCy tokenization\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]  # Remove stopwords and punctuation\n",
    "\n",
    "    # Generate n-grams\n",
    "    ngrams_list = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "# Example usage\n",
    "text = \"Real-time animation of deformable objects is always a compromise.\"\n",
    "unigrams = generate_ngrams(text, 1)\n",
    "bigrams = generate_ngrams(text, 2)\n",
    "trigrams = generate_ngrams(text, 3)\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "\n",
    "# 1. N-gram generation\n",
    "#def generate_ngrams(text, n=1):\n",
    "#    tokens = nltk.word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "#    return list(ngrams(tokens, n))\n",
    "\n",
    "# 2. Extract Modal Verbs using spaCy\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = [token.text for token in doc if token.tag_ == 'MD']  # Modal verbs in spaCy are tagged as 'MD'\n",
    "    return ' '.join(modal_verbs)\n",
    "\n",
    "# 3. Detect Negations\n",
    "def detect_negation(text):\n",
    "    doc = nlp(text)\n",
    "    negations = [token.text for token in doc if token.dep_ == 'neg']\n",
    "    return ' '.join(negations)\n",
    "\n",
    "# 4. Argument lexicon match (agreement and disagreement)\n",
    "def check_argument_lexicon(text):\n",
    "    tokens = text.lower().split()\n",
    "    agreement_count = sum(1 for word in tokens if word in agreement_lexicon)\n",
    "    disagreement_count = sum(1 for word in tokens if word in disagreement_lexicon)\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# 5. Generate feature vector\n",
    "def extract_features(text, label):\n",
    "    # 1. N-grams (Unigrams, Bigrams, Trigrams)\n",
    "    unigrams = generate_ngrams(text, n=1)\n",
    "    bigrams = generate_ngrams(text, n=2)\n",
    "    trigrams = generate_ngrams(text, n=3)\n",
    "\n",
    "    # Flatten the ngrams into strings\n",
    "    unigram_str = ' '.join([''.join(gram) for gram in unigrams])\n",
    "    bigram_str = ' '.join([''.join(gram) for gram in bigrams])\n",
    "    trigram_str = ' '.join([''.join(gram) for gram in trigrams])\n",
    "\n",
    "    # 2. Argument lexicons (agreement and disagreement)\n",
    "    agreement_count, disagreement_count = check_argument_lexicon(text)\n",
    "\n",
    "    # 3. Modal verbs\n",
    "    modals = extract_modal_verbs(text)\n",
    "\n",
    "    # 4. Negations\n",
    "    negations = detect_negation(text)\n",
    "\n",
    "    # Return the feature vector\n",
    "    return [text, label, unigram_str, bigram_str, trigram_str, agreement_count, disagreement_count, modals, negations]\n",
    "\n",
    "# Function to process compiled_output.csv and extract features\n",
    "def process_and_extract_features(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        header = next(reader)  # Skip the header row\n",
    "        data = list(reader)\n",
    "    \n",
    "    # Open the output CSV to write features\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Text', 'Label', 'Unigrams', 'Bigrams', 'Trigrams', 'Agreement_Count', 'Disagreement_Count', 'Modal_Verbs', 'Negations'])\n",
    "        \n",
    "        for row in data:\n",
    "            text, label = row\n",
    "            features = extract_features(text, label)\n",
    "            writer.writerow(features)\n",
    "\n",
    "# Example usage\n",
    "input_csv = 'compiled_output.csv'  # Path to your compiled output CSV file\n",
    "output_csv = 'extracted_features.csv'  # Path to the output CSV with features\n",
    "\n",
    "process_and_extract_features(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
