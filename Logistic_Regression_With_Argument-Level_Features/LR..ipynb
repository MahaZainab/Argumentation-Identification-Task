{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"compiled_output.csv\")\n",
    "\n",
    "# Display the first few rows to check the structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or empty text\n",
    "df['Text'].isnull().sum()  # Check for NaNs\n",
    "df[df['Text'].str.strip() == '']  # Check for empty or all-whitespace texts\n",
    "# Remove rows where text is empty or null\n",
    "df = df[df['Text'].str.strip() != '']\n",
    "df = df.dropna(subset=['Text'])\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: complicated 3D character models are widely used in fields of entertainment, virtual reality, medicine etc\n",
      "Tokenized Text: ['complicated', '3D', 'character', 'models', 'are', 'widely', 'used', 'in', 'fields', 'of', 'entertainment,', 'virtual', 'reality,', 'medicine', 'etc']\n"
     ]
    }
   ],
   "source": [
    "# Test tokenization of a sample text\n",
    "sample_text = df['Text'].iloc[0]\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Tokenized Text:\", sample_text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['3d' '3d character' '3d character models' '3d models' '3d models limited'\n",
      " 'afford' 'afford major' 'afford major revisions' 'animation'\n",
      " 'animation remains' 'animation remains open' 'approach'\n",
      " 'approach character' 'approach character skinning' 'artists'\n",
      " 'artists resolution' 'artists resolution devices' 'breathtaking'\n",
      " 'breathtaking realistic' 'breathtaking realistic 3d' 'character'\n",
      " 'character models' 'character models widely' 'character skinning'\n",
      " 'character skinning present' 'complicated' 'complicated 3d'\n",
      " 'complicated 3d character' 'creativity' 'creativity artists'\n",
      " 'creativity artists resolution' 'deformation' 'deformation ssd'\n",
      " 'deformation ssd predominant' 'devices' 'efficient' 'efficient solution'\n",
      " 'efficient solution animation' 'entertainment' 'entertainment virtual'\n",
      " 'entertainment virtual reality' 'fields' 'fields entertainment'\n",
      " 'fields entertainment virtual' 'flexible' 'flexible efficient'\n",
      " 'flexible efficient solution' 'limited' 'limited creativity'\n",
      " 'limited creativity artists' 'major' 'major revisions' 'medicine'\n",
      " 'models' 'models limited' 'models limited creativity' 'models widely'\n",
      " 'models widely used' 'open' 'open problem' 'predominant'\n",
      " 'predominant approach' 'predominant approach character' 'present'\n",
      " 'problem' 'production' 'production afford' 'production afford major'\n",
      " 'providing' 'providing flexible' 'providing flexible efficient' 'range'\n",
      " 'range breathtaking' 'range breathtaking realistic' 'realistic'\n",
      " 'realistic 3d' 'realistic 3d models' 'reality' 'reality medicine'\n",
      " 'remains' 'remains open' 'remains open problem' 'resolution'\n",
      " 'resolution devices' 'revisions' 'skeleton' 'skeleton subspace'\n",
      " 'skeleton subspace deformation' 'skinning' 'skinning present' 'solution'\n",
      " 'solution animation' 'solution animation remains' 'ssd' 'ssd predominant'\n",
      " 'ssd predominant approach' 'subspace' 'subspace deformation'\n",
      " 'subspace deformation ssd' 'used' 'used fields'\n",
      " 'used fields entertainment' 'virtual' 'virtual reality'\n",
      " 'virtual reality medicine' 'widely' 'widely used' 'widely used fields']\n"
     ]
    }
   ],
   "source": [
    "# Test extracting n-grams from a small set of text\n",
    "sample_texts = df['Text'].head(5)  # First 5 rows for quick testing\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_test = vectorizer.fit_transform(sample_texts)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Argument lexicons\n",
    "agreement_lexicon = ['agree', 'yes', 'definitely', 'sure', 'absolutely', 'of course']\n",
    "disagreement_lexicon = ['disagree', 'no', 'never', 'not', 'don’t', 'won’t']\n",
    "\n",
    "# Hedge words\n",
    "hedge_words = ['perhaps', 'maybe', 'possibly', 'could', 'might', 'probably']\n",
    "\n",
    "# Modal verbs list\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract Argument Lexicons (Agreement and Disagreement)\n",
    "def extract_argument_lexicon_features(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# Extract Hedge Features\n",
    "def extract_hedge_features(text):\n",
    "    hedge_count = sum([word in text.lower() for word in hedge_words])\n",
    "    return hedge_count\n",
    "\n",
    "# Extract Modal Verbs\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return modal_count\n",
    "\n",
    "# Detect Negation\n",
    "def detect_negation(text):\n",
    "    negation_patterns = [r'\\b(not|no|never|don\\'t|won\\'t|isn\\'t|aren\\'t|can\\'t)\\b']\n",
    "    negation_count = sum([bool(re.search(pattern, text.lower())) for pattern in negation_patterns])\n",
    "    return negation_count\n",
    "\n",
    "# Function to extract all features for each text\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract n-grams and other features\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract n-grams and other features\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Argument lexicons\n",
    "agreement_lexicon = ['agree', 'yes', 'definitely', 'sure', 'absolutely', 'of course']\n",
    "disagreement_lexicon = ['disagree', 'no', 'never', 'not', 'don’t', 'won’t']\n",
    "\n",
    "# Hedge words\n",
    "hedge_words = ['perhaps', 'maybe', 'possibly', 'could', 'might', 'probably']\n",
    "\n",
    "# Modal verbs list\n",
    "modal_verbs = ['can', 'could', 'may', 'might', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# Function to extract n-grams\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Check if text is non-empty and not just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        return vectorizer.get_feature_names_out()\n",
    "    else:\n",
    "        return []  # Return an empty list for empty text\n",
    "\n",
    "# Extract Argument Lexicons (Agreement and Disagreement)\n",
    "def extract_argument_lexicon_features(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return agreement_count, disagreement_count\n",
    "\n",
    "# Extract Hedge Features\n",
    "def extract_hedge_features(text):\n",
    "    hedge_count = sum([word in text.lower() for word in hedge_words])\n",
    "    return hedge_count\n",
    "\n",
    "# Extract Modal Verbs\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return modal_count\n",
    "\n",
    "# Detect Negation\n",
    "def detect_negation(text):\n",
    "    negation_patterns = [r'\\b(not|no|never|don\\'t|won\\'t|isn\\'t|aren\\'t|can\\'t)\\b']\n",
    "    negation_count = sum([bool(re.search(pattern, text.lower())) for pattern in negation_patterns])\n",
    "    return negation_count\n",
    "\n",
    "# Function to extract all features for each text\n",
    "def extract_features(df):\n",
    "    all_ngrams = []\n",
    "    other_features = []\n",
    "    \n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features = list(ngrams)  # Get the ngram features\n",
    "        \n",
    "        # Extract argument lexicons (agreement, disagreement)\n",
    "        agreement_count, disagreement_count = extract_argument_lexicon_features(text)\n",
    "\n",
    "        # Extract hedge features\n",
    "        hedge_count = extract_hedge_features(text)\n",
    "\n",
    "        # Extract modal verbs\n",
    "        modal_count = extract_modal_verbs(text)\n",
    "\n",
    "        # Extract negation features\n",
    "        negation_count = detect_negation(text)\n",
    "\n",
    "        # Combine all the features into one list\n",
    "        other_features.append([agreement_count, disagreement_count, hedge_count, modal_count, negation_count])\n",
    "\n",
    "        # Store the ngram features as a separate part of the feature matrix\n",
    "        all_ngrams.append(ngram_features)\n",
    "    \n",
    "    return all_ngrams, other_features\n",
    "\n",
    "# Now, proceed with the previous steps to create feature matrix and train Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Combine n-grams and additional features into a single feature matrix\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# N-grams will be a list of list of n-grams, so we need to flatten this into a single feature vector for each entry\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     13\u001b[0m other_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Extract n-grams\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     ngram_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ngrams)  \u001b[38;5;66;03m# Get the ngram features\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Extract argument lexicons (agreement, disagreement)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Check if text is non-empty and not just whitespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract features and labels\n",
    "#df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams and additional features into a single feature matrix\n",
    "# N-grams will be a list of list of n-grams, so we need to flatten this into a single feature vector for each entry\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "count    13592.000000\n",
      "mean        59.892216\n",
      "std         45.405939\n",
      "min          1.000000\n",
      "25%         22.000000\n",
      "50%         53.000000\n",
      "75%         86.000000\n",
      "max        359.000000\n",
      "Name: Text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'].isnull().sum())  # Check for any null values\n",
    "print(df['Text'].apply(len).describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m\n\u001b[0;32m     85\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     91\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[2], line 60\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Initialize vectorizer without stopwords\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Ensure the text is not empty or consisting of just spaces\n",
    "    if text.strip():\n",
    "        # Initialize vectorizer without stopwords\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words=None)\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words=None)\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     89\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Ensure the text is not empty or just whitespace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Ensure the text is not empty or just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     89\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \u001b[38;5;66;03m# Ensure the text is not empty or just whitespace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# If ngrams contain valid features, return them, otherwise return empty\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    if text.strip():  # Ensure the text is not empty or just whitespace\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If ngrams contain valid features, return them, otherwise return empty\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty list if no n-grams are found\n",
    "    else:\n",
    "        return []  # Return empty list for empty or invalid text\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_2952\\856623152.py\", line 7, in <module>\n",
      "    import spacy\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n",
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n",
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_Length\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m    109\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[1], line 59\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Extract additional features\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     argument_lexicons \u001b[38;5;241m=\u001b[39m extract_argument_lexicons(text)\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():  \n\u001b[0;32m     17\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# If no valid n-grams are found, return empty list\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load the Spacy model for modal verb detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for extracting n-grams (unigrams, bigrams, trigrams)\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Check if the text is valid (non-empty after removing spaces)\n",
    "    if text.strip():  \n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # If no valid n-grams are found, return empty list\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return ['<empty_ngram>']  # Return a placeholder for empty ngrams\n",
    "    else:\n",
    "        return ['<empty_ngram>']  # Return a placeholder for empty ngrams\n",
    "\n",
    "# Example lexicons for argument extraction (these could be refined further)\n",
    "agreement_lexicon = ['agree', 'agreed', 'agreement', 'yes', 'support']\n",
    "disagreement_lexicon = ['disagree', 'disagreed', 'disagreement', 'no', 'oppose']\n",
    "\n",
    "# Function to extract argument lexicons (agreement and disagreement)\n",
    "def extract_argument_lexicons(text):\n",
    "    agreement_count = sum([word in text.lower() for word in agreement_lexicon])\n",
    "    disagreement_count = sum([word in text.lower() for word in disagreement_lexicon])\n",
    "    return [agreement_count, disagreement_count]\n",
    "\n",
    "# Function to detect modal verbs (e.g., \"can\", \"could\", \"will\", etc.)\n",
    "def extract_modal_verbs(text):\n",
    "    doc = nlp(text)\n",
    "    modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'ought']\n",
    "    modal_count = sum([token.lemma_ in modal_verbs for token in doc])\n",
    "    return [modal_count]\n",
    "\n",
    "# Function to detect negation words (e.g., \"not\", \"never\", etc.)\n",
    "def extract_negation(text):\n",
    "    negation_words = ['not', 'never', 'no', 'none', 'nothing', 'neither', 'nor']\n",
    "    negation_count = sum([word in text.lower() for word in negation_words])\n",
    "    return [negation_count]\n",
    "\n",
    "# Function to extract all features\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    other_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        \n",
    "        # Extract additional features\n",
    "        argument_lexicons = extract_argument_lexicons(text)\n",
    "        modal_verbs = extract_modal_verbs(text)\n",
    "        negation = extract_negation(text)\n",
    "        \n",
    "        # Combine all features into a single list for this text\n",
    "        ngram_features.append(ngrams)\n",
    "        other_features.append(argument_lexicons + modal_verbs + negation)\n",
    "    \n",
    "    return ngram_features, other_features\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Check the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Remove rows where the 'Text' column contains only numbers or lists (e.g., [2])\n",
    "df = df[~df['Text'].str.match(r'^\\[.*\\]$')]  # Regex to match rows like '[2]', '[text]', etc.\n",
    "\n",
    "# Alternatively, if you want to remove rows where 'Text' only contains digits or lists of digits\n",
    "df = df[~df['Text'].str.match(r'^\\[\\d+\\]$')]  # Regex to match rows like '[2]', '[3]', etc.\n",
    "\n",
    "# Verify the result\n",
    "print(df.head())\n",
    "\n",
    "# Now you can continue with your feature extraction or other operations\n",
    "\n",
    "\n",
    "# Remove rows with NaN or empty Text after stripping extra spaces\n",
    "df = df[df['Text'].notna() & (df['Text'].str.strip() != '')]\n",
    "df['Text'] = df['Text'].str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Remove rows with empty or very short texts (length < 3 words)\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df = df[df['Text_Length'] > 2]\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')  # Added stop_words='english' here\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ngram_features, additional_features\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     36\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     ngram_features\u001b[38;5;241m.\u001b[39mappend(ngrams)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Here you can add your other feature extraction methods\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# For now, just add a placeholder for additional features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Initialize vectorizer with stop_words removed\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Check if any valid ngrams were extracted\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Only proceed if the text is not empty after stripping\n",
    "    if text.strip():\n",
    "        # Initialize vectorizer with stop_words removed\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # Check if any valid ngrams were extracted\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty if no valid ngrams were found\n",
    "    return []  # Return empty if the text is invalid or empty\n",
    "\n",
    "# Function to extract features from the dataframe\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    additional_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features.append(ngrams)\n",
    "\n",
    "        # Here you can add your other feature extraction methods\n",
    "        # For now, just add a placeholder for additional features\n",
    "        additional_features.append([])  # Replace with actual feature extraction logic\n",
    "\n",
    "    return ngram_features, additional_features\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "import numpy as np\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after cleaning:\n",
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n",
      "\n",
      "Data after removing non-informative content:\n",
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n",
      "\n",
      "Remaining rows after length filter: 11288\n",
      "                                                Text             Label  \\\n",
      "0  complicated 3D character models are widely use...  background_claim   \n",
      "1  The range of breathtaking realistic 3D models ...  background_claim   \n",
      "2         a production cannot afford major revisions  background_claim   \n",
      "3  providing a flexible and efficient solution to...         own_claim   \n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim   \n",
      "\n",
      "   Text_Length  \n",
      "0           15  \n",
      "1           19  \n",
      "2            6  \n",
      "3           12  \n",
      "4           13  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ngram_features, additional_features\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m ngram_features, additional_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Combine n-grams into one string for each row to pass into vectorizer\u001b[39;00m\n\u001b[0;32m     66\u001b[0m ngram_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ngram) \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m ngram_features]\n",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Loop through each text entry in the dataframe\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Extract n-grams (unigrams, bigrams, trigrams)\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mextract_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     ngram_features\u001b[38;5;241m.\u001b[39mappend(ngrams)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Here you can add your other feature extraction methods\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# For now, just add a placeholder for additional features\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mextract_ngrams\u001b[1;34m(text, ngram_range)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Initialize vectorizer with stop_words removed\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39mngram_range, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m     ngrams \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Check if any valid ngrams were extracted\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ngrams\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Step 1: Remove rows where 'Text' is empty, whitespace, or non-informative\n",
    "df = df[df['Text'].str.strip().notna()]  # Remove rows with empty strings\n",
    "df = df[df['Text'].str.strip() != '']  # Remove rows with only whitespace\n",
    "\n",
    "# Print the first few rows to check\n",
    "print(\"Data after cleaning:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Check for rows with non-informative content like `[2]`\n",
    "# This will help you filter out any non-textual or irrelevant rows (e.g., '[2]')\n",
    "df = df[~df['Text'].str.contains(r'\\[.*\\]')]  # Remove rows with square brackets (e.g., '[2]')\n",
    "print(\"\\nData after removing non-informative content:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Filter out rows where the text is too short to contain meaningful n-grams\n",
    "df['Text_Length'] = df['Text'].apply(lambda x: len(x.split()))  # Length in words\n",
    "df = df[df['Text_Length'] > 2]  # Only keep rows with more than 2 words\n",
    "\n",
    "# Check if there are still any rows left after filtering\n",
    "print(f\"\\nRemaining rows after length filter: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 4: Function to extract n-grams safely\n",
    "def extract_ngrams(text, ngram_range=(1, 3)):\n",
    "    # Only proceed if the text is not empty after stripping\n",
    "    if text.strip():\n",
    "        # Initialize vectorizer with stop_words removed\n",
    "        vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "        ngrams = vectorizer.fit_transform([text])\n",
    "        \n",
    "        # Check if any valid ngrams were extracted\n",
    "        if ngrams.shape[1] > 0:\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        else:\n",
    "            return []  # Return empty if no valid ngrams were found\n",
    "    return []  # Return empty if the text is invalid or empty\n",
    "\n",
    "# Function to extract features from the dataframe\n",
    "def extract_features(df):\n",
    "    ngram_features = []\n",
    "    additional_features = []\n",
    "\n",
    "    # Loop through each text entry in the dataframe\n",
    "    for text in df['Text']:\n",
    "        # Extract n-grams (unigrams, bigrams, trigrams)\n",
    "        ngrams = extract_ngrams(text)\n",
    "        ngram_features.append(ngrams)\n",
    "\n",
    "        # Here you can add your other feature extraction methods\n",
    "        # For now, just add a placeholder for additional features\n",
    "        additional_features.append([])  # Replace with actual feature extraction logic\n",
    "\n",
    "    return ngram_features, additional_features\n",
    "\n",
    "# Extract features and labels\n",
    "ngram_features, additional_features = extract_features(df)\n",
    "\n",
    "# Combine n-grams into one string for each row to pass into vectorizer\n",
    "ngram_texts = [' '.join(ngram) for ngram in ngram_features]\n",
    "\n",
    "# Create the CountVectorizer for n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "X_ngrams = vectorizer.fit_transform(ngram_texts)\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(additional_features)\n",
    "\n",
    "# Combine n-grams and additional features (horizontal stack)\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_ngrams, X_additional])\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after filtering: 13454\n",
      "                                                Text             Label\n",
      "0  complicated 3D character models are widely use...  background_claim\n",
      "1  The range of breathtaking realistic 3D models ...  background_claim\n",
      "2         a production cannot afford major revisions  background_claim\n",
      "3  providing a flexible and efficient solution to...         own_claim\n",
      "4  Skeleton Subspace Deformation (SSD) is the pre...  background_claim\n",
      "Logistic Regression Accuracy: 42.93%\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "background_claim       0.00      0.00      0.00      1039\n",
      "            data       0.00      0.00      0.00      1265\n",
      "       own_claim       0.43      1.00      0.60      1733\n",
      "\n",
      "        accuracy                           0.43      4037\n",
      "       macro avg       0.14      0.33      0.20      4037\n",
      "    weighted avg       0.18      0.43      0.26      4037\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\PC\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model for modal verb and negation detection\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a list of agreement and disagreement words\n",
    "agreement_lexicon = ['agree', 'agreement', 'consistent', 'consistent with', 'support', 'supports', 'endorses']\n",
    "disagreement_lexicon = ['disagree', 'disagreement', 'opposes', 'opposed', 'against', 'contradicts']\n",
    "\n",
    "# Define a list of hedge words\n",
    "hedge_lexicon = ['perhaps', 'maybe', 'likely', 'uncertain', 'possibly', 'could', 'should']\n",
    "\n",
    "# Define a list of negation words\n",
    "negation_lexicon = ['not', 'never', 'no', 'none', 'nothing', 'neither']\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('compiled_output.csv')  # Replace with your file path\n",
    "\n",
    "# Step 1: Remove rows with empty or irrelevant content\n",
    "df = df[df['Text'].str.strip().notna()]  # Remove rows with empty strings\n",
    "df = df[df['Text'].str.strip() != '']  # Remove rows with only whitespace\n",
    "df = df[~df['Text'].str.contains(r'\\[.*\\]')]  # Remove rows with non-textual content like '[2]'\n",
    "\n",
    "# Step 2: Check if there are still any rows left after filtering\n",
    "print(f\"Remaining rows after filtering: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "def extract_features(df):\n",
    "    agreement_features = []\n",
    "    disagreement_features = []\n",
    "    hedge_features = []\n",
    "    negation_features = []\n",
    "    modal_verbs_features = []\n",
    "\n",
    "    for text in df['Text']:\n",
    "        # Tokenize the text using spaCy for modal verbs and negation\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Feature 1: Check for agreement and disagreement lexicons\n",
    "        agreement_score = sum(1 for word in text.lower().split() if word in agreement_lexicon)\n",
    "        disagreement_score = sum(1 for word in text.lower().split() if word in disagreement_lexicon)\n",
    "\n",
    "        # Feature 2: Check for hedge words\n",
    "        hedge_score = sum(1 for word in text.lower().split() if word in hedge_lexicon)\n",
    "\n",
    "        # Feature 3: Check for negation words\n",
    "        negation_score = sum(1 for word in text.lower().split() if word in negation_lexicon)\n",
    "\n",
    "        # Feature 4: Check for modal verbs using spaCy's part-of-speech tagging\n",
    "        modal_verbs_score = sum(1 for token in doc if token.pos_ == 'VERB' and token.morph.get('Mood') == ['Ind'] and token.dep_ == 'aux')\n",
    "\n",
    "        # Append the features to respective lists\n",
    "        agreement_features.append(agreement_score)\n",
    "        disagreement_features.append(disagreement_score)\n",
    "        hedge_features.append(hedge_score)\n",
    "        negation_features.append(negation_score)\n",
    "        modal_verbs_features.append(modal_verbs_score)\n",
    "\n",
    "    # Combine all features into a single matrix\n",
    "    features = np.array([agreement_features, disagreement_features, hedge_features, negation_features, modal_verbs_features]).T\n",
    "    return features\n",
    "\n",
    "# Step 4: Extract the features\n",
    "X = extract_features(df)\n",
    "\n",
    "# Prepare the target labels (Y)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Step 5: Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 6: Train a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Predict the labels on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 9: Print the classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
